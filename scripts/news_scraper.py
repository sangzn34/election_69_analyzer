#!/usr/bin/env python3
"""
Election 69 News Scraper
========================
Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á 2569 ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß ‡πÅ‡∏•‡πâ‡∏ß export ‡πÄ‡∏õ‡πá‡∏ô JSON
‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡πÉ‡∏™‡πà ElectionNews.tsx ‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å

Usage:
    python scripts/news_scraper.py
    python scripts/news_scraper.py --pages 3
    python scripts/news_scraper.py --source khaosod
    python scripts/news_scraper.py --keyword "‡∏ö‡∏≤‡∏£‡πå‡πÇ‡∏Ñ‡πâ‡∏î"
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import re
import time
import argparse
from datetime import datetime
from urllib.parse import urljoin, quote

# ‚îÄ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "..", "data", "news")
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "scraped_news.json")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "th,en;q=0.9",
}

# ‡∏Ç‡πà‡∏≤‡∏ß keyword ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á
ELECTION_KEYWORDS = [
    "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á", "‡∏Å‡∏Å‡∏ï", "‡∏ö‡∏±‡∏ï‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á", "‡∏ô‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏ö‡∏≤‡∏£‡πå‡πÇ‡∏Ñ‡πâ‡∏î",
    "‡∏´‡∏µ‡∏ö‡∏ö‡∏±‡∏ï‡∏£", "‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï", "‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á", "‡πÇ‡∏°‡∏Ü‡∏∞", "‡∏ö‡∏±‡∏ï‡∏£‡πÄ‡∏Ç‡∏¢‡πà‡∏á",
    "‡∏´‡∏±‡∏ß‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£ ‡∏™‡∏™", "‡∏õ‡∏≤‡∏£‡πå‡∏ï‡∏µ‡πâ‡∏•‡∏¥‡∏™‡∏ï‡πå", "‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á",
    "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á", "‡∏ú‡∏•‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á", "‡∏Ñ‡∏π‡∏´‡∏≤", "‡∏ï‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ß", "QR Code",
    "‡∏®‡∏≤‡∏•‡∏£‡∏ò‡∏ô", "‡∏®‡∏≤‡∏•‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç", "‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô",
    "‡∏û‡∏£‡∏£‡∏Ñ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô", "‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏à‡πÑ‡∏ó‡∏¢", "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢", "‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ò‡∏¥‡∏õ‡∏±‡∏ï‡∏¢‡πå",
    "‡∏Å‡∏•‡πâ‡∏≤‡∏ò‡∏£‡∏£‡∏°", "‡πÑ‡∏ó‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ó‡∏¢", "‡∏û‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏£‡∏±‡∏ê",
]

REQUEST_DELAY = 1.5  # seconds between requests


# ‚îÄ‚îÄ‚îÄ Helper Functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def safe_request(url, max_retries=3):
    """Make HTTP request with retry logic."""
    for attempt in range(max_retries):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=15)
            resp.raise_for_status()
            return resp
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait = 2 ** attempt
                print(f"  ‚ö†Ô∏è  Retry {attempt+1}/{max_retries} for {url} (wait {wait}s)")
                time.sleep(wait)
            else:
                print(f"  ‚ùå Failed after {max_retries} attempts: {url}")
                print(f"     Error: {e}")
                return None


def is_election_related(title, summary=""):
    """Check if article is related to elections."""
    text = (title + " " + summary).lower()
    return any(kw.lower() in text for kw in ELECTION_KEYWORDS)


def thai_date_to_standard(thai_date_str):
    """
    Convert Thai date formats to DD/MM/YYYY (Buddhist era).
    Handles: "13 ‡∏Å.‡∏û. 2569", "13 ‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå 2569", "2026-02-13" etc.
    """
    thai_months = {
        "‡∏°.‡∏Ñ.": "01", "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°": "01",
        "‡∏Å.‡∏û.": "02", "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå": "02",
        "‡∏°‡∏µ.‡∏Ñ.": "03", "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°": "03",
        "‡πÄ‡∏°.‡∏¢.": "04", "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô": "04",
        "‡∏û.‡∏Ñ.": "05", "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°": "05",
        "‡∏°‡∏¥.‡∏¢.": "06", "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô": "06",
        "‡∏Å.‡∏Ñ.": "07", "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°": "07",
        "‡∏™.‡∏Ñ.": "08", "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°": "08",
        "‡∏Å.‡∏¢.": "09", "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô": "09",
        "‡∏ï.‡∏Ñ.": "10", "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°": "10",
        "‡∏û.‡∏¢.": "11", "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô": "11",
        "‡∏ò.‡∏Ñ.": "12", "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°": "12",
    }

    # Try Thai format: "13 ‡∏Å.‡∏û. 2569" or "13 ‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå 2569"
    for thai_m, num_m in thai_months.items():
        if thai_m in thai_date_str:
            parts = thai_date_str.strip().split()
            day = parts[0].strip()
            # Find year (4-digit number)
            year = None
            for p in parts:
                p_clean = p.strip().rstrip(".")
                if re.match(r"^\d{4}$", p_clean):
                    year = p_clean
                    break
            if day and year:
                return f"{int(day):02d}/{num_m}/{year}"

    # Try ISO format: "2026-02-13"
    iso_match = re.search(r"(\d{4})-(\d{2})-(\d{2})", thai_date_str)
    if iso_match:
        y, m, d = iso_match.groups()
        be_year = int(y) + 543
        return f"{d}/{m}/{be_year}"

    return thai_date_str


def extract_time(text):
    """Extract time (HH:MM) from text like '13 ‡∏Å.‡∏û. 2569 17:06 ‡∏ô.' or '10:39 ‡∏ô.'"""
    time_match = re.search(r"(\d{1,2}):(\d{2})\s*‡∏ô\.?", text)
    if time_match:
        h, m = time_match.groups()
        return f"{int(h):02d}:{m}"
    # Also try HH.MM format
    time_match = re.search(r"(\d{1,2})\.(\d{2})\s*‡∏ô\.?", text)
    if time_match:
        h, m = time_match.groups()
        return f"{int(h):02d}:{m}"
    return None


# ‚îÄ‚îÄ‚îÄ Scrapers for each source ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def scrape_khaosod(pages=2, keyword=None):
    """
    Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏î (khaosod.co.th)
    - /politics section
    - Search results if keyword provided
    """
    articles = []
    source_name = "‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏î"

    if keyword:
        # Search mode
        urls = [
            f"https://www.khaosod.co.th/search?q={quote(keyword)}&page={p}"
            for p in range(1, pages + 1)
        ]
    else:
        urls = [
            f"https://www.khaosod.co.th/politics?page={p}"
            for p in range(1, pages + 1)
        ]

    for url in urls:
        print(f"  üì∞ Fetching: {url}")
        resp = safe_request(url)
        if not resp:
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        # Find article links
        article_links = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "/news_" in href and href.startswith("http"):
                article_links.add(href)
            elif "/news_" in href:
                article_links.add(urljoin("https://www.khaosod.co.th", href))

        print(f"    Found {len(article_links)} article links")

        for link in article_links:
            time.sleep(REQUEST_DELAY)
            article = scrape_khaosod_article(link, source_name)
            if article:
                articles.append(article)

    return articles


def scrape_khaosod_article(url, source_name="‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏î"):
    """Scrape a single Khaosod article."""
    resp = safe_request(url)
    if not resp:
        return None

    soup = BeautifulSoup(resp.text, "html.parser")

    # Title
    title_el = soup.find("h1")
    if not title_el:
        return None
    title = title_el.get_text(strip=True)

    if not is_election_related(title):
        print(f"    ‚è≠Ô∏è  Skip (not election): {title[:50]}...")
        return None

    # Date/Time ‚Äî look for pattern like "13 ‡∏Å.‡∏û. 2569-12:45 ‡∏ô."
    date_str = ""
    time_str = ""

    # Try meta tags first
    meta_date = soup.find("meta", {"property": "article:published_time"})
    if meta_date:
        content = meta_date.get("content", "")
        date_str = thai_date_to_standard(content)
        # Extract time from ISO format
        t_match = re.search(r"T(\d{2}):(\d{2})", content)
        if t_match:
            time_str = f"{t_match.group(1)}:{t_match.group(2)}"

    # Fallback: look in page text
    if not date_str:
        date_els = soup.find_all(string=re.compile(r"\d{1,2}\s+(‡∏°\.‡∏Ñ\.|‡∏Å\.‡∏û\.|‡∏°‡∏µ\.‡∏Ñ\.|‡πÄ‡∏°\.‡∏¢\.|‡∏û\.‡∏Ñ\.|‡∏°‡∏¥\.‡∏¢\.|‡∏Å\.‡∏Ñ\.|‡∏™\.‡∏Ñ\.|‡∏Å\.‡∏¢\.|‡∏ï\.‡∏Ñ\.|‡∏û\.‡∏¢\.|‡∏ò\.‡∏Ñ\.)\s+\d{4}"))
        for el in date_els:
            text = el.strip()
            date_str = thai_date_to_standard(text)
            if not time_str:
                time_str = extract_time(text) or ""
            if date_str:
                break

    # Summary ‚Äî first paragraph or meta description
    summary = ""
    meta_desc = soup.find("meta", {"name": "description"})
    if meta_desc:
        summary = meta_desc.get("content", "").strip()

    if not summary:
        # Try first <p> in article body
        article_body = soup.find("div", class_=re.compile(r"article|content|entry"))
        if article_body:
            first_p = article_body.find("p")
            if first_p:
                summary = first_p.get_text(strip=True)[:300]

    # Tags
    tags = []
    tag_els = soup.find_all("a", href=re.compile(r"/tag/"))
    for tag_el in tag_els:
        tag_text = tag_el.get_text(strip=True)
        if tag_text and len(tag_text) < 50:
            tags.append(tag_text)

    print(f"    ‚úÖ {title[:60]}...")

    return {
        "title": title,
        "summary": summary[:500] if summary else "",
        "date": date_str,
        "time": time_str,
        "source": source_name,
        "sourceUrl": url,
        "tags": tags[:8],
    }


def scrape_thairath(pages=2, keyword=None):
    """
    Scrape ‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê (thairath.co.th)
    - /news/politic section
    """
    articles = []
    source_name = "‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê"

    if keyword:
        urls = [
            f"https://www.thairath.co.th/search?q={quote(keyword)}&page={p}"
            for p in range(1, pages + 1)
        ]
    else:
        urls = [
            f"https://www.thairath.co.th/news/politic"
        ]
        # Thairath uses infinite scroll, so we mainly get the first page
        # plus search if keyword given

    for url in urls:
        print(f"  üì∞ Fetching: {url}")
        resp = safe_request(url)
        if not resp:
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        article_links = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            # Match /news/politic/XXXXXXX pattern
            if re.search(r"/news/politic/\d+", href):
                full_url = urljoin("https://www.thairath.co.th", href)
                article_links.add(full_url)
            # Also match /video/news-update/XXXXXXX
            elif re.search(r"/video/.*?/\d+", href):
                full_url = urljoin("https://www.thairath.co.th", href)
                article_links.add(full_url)

        print(f"    Found {len(article_links)} article links")

        for link in article_links:
            time.sleep(REQUEST_DELAY)
            article = scrape_thairath_article(link, source_name)
            if article:
                articles.append(article)

    return articles


def scrape_thairath_article(url, source_name="‡πÑ‡∏ó‡∏¢‡∏£‡∏±‡∏ê"):
    """Scrape a single Thairath article."""
    resp = safe_request(url)
    if not resp:
        return None

    soup = BeautifulSoup(resp.text, "html.parser")

    # Title
    title_el = soup.find("h1")
    if not title_el:
        return None
    title = title_el.get_text(strip=True)

    if not is_election_related(title):
        print(f"    ‚è≠Ô∏è  Skip (not election): {title[:50]}...")
        return None

    # Date/Time
    date_str = ""
    time_str = ""

    # Try meta tag
    meta_date = soup.find("meta", {"property": "article:published_time"})
    if meta_date:
        content = meta_date.get("content", "")
        date_str = thai_date_to_standard(content)
        t_match = re.search(r"T(\d{2}):(\d{2})", content)
        if t_match:
            time_str = f"{t_match.group(1)}:{t_match.group(2)}"

    # Fallback: look for visible date text
    if not date_str:
        for el in soup.find_all(string=re.compile(r"\d{1,2}\s+‡∏Å\.‡∏û\.\s+\d{4}")):
            text = el.strip()
            date_str = thai_date_to_standard(text)
            time_str = extract_time(text) or ""
            if date_str:
                break

    # Summary
    summary = ""
    meta_desc = soup.find("meta", {"name": "description"})
    if meta_desc:
        summary = meta_desc.get("content", "").strip()

    # Tags ‚Äî Thairath uses tag links
    tags = []
    for tag_a in soup.find_all("a", href=re.compile(r"/tag/")):
        t = tag_a.get_text(strip=True)
        if t and len(t) < 50:
            tags.append(t)

    print(f"    ‚úÖ {title[:60]}...")

    return {
        "title": title,
        "summary": summary[:500] if summary else "",
        "date": date_str,
        "time": time_str,
        "source": source_name,
        "sourceUrl": url,
        "tags": tags[:8],
    }


def scrape_matichon(pages=2, keyword=None):
    """
    Scrape ‡∏°‡∏ï‡∏¥‡∏ä‡∏ô (matichon.co.th)
    - /politics/election69 section
    """
    articles = []
    source_name = "‡∏°‡∏ï‡∏¥‡∏ä‡∏ô"

    if keyword:
        urls = [
            f"https://www.matichon.co.th/?s={quote(keyword)}&paged={p}"
            for p in range(1, pages + 1)
        ]
    else:
        urls = [
            f"https://www.matichon.co.th/politics/election69?paged={p}"
            for p in range(1, pages + 1)
        ]

    for url in urls:
        print(f"  üì∞ Fetching: {url}")
        resp = safe_request(url)
        if not resp:
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        article_links = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if re.search(r"/news_\d+", href):
                full_url = urljoin("https://www.matichon.co.th", href)
                article_links.add(full_url)

        print(f"    Found {len(article_links)} article links")

        for link in article_links:
            time.sleep(REQUEST_DELAY)
            article = scrape_matichon_article(link, source_name)
            if article:
                articles.append(article)

    return articles


def scrape_matichon_article(url, source_name="‡∏°‡∏ï‡∏¥‡∏ä‡∏ô"):
    """Scrape a single Matichon article."""
    resp = safe_request(url)
    if not resp:
        return None

    soup = BeautifulSoup(resp.text, "html.parser")

    # Title
    title_el = soup.find("h1")
    if not title_el:
        return None
    title = title_el.get_text(strip=True)

    if not is_election_related(title):
        print(f"    ‚è≠Ô∏è  Skip (not election): {title[:50]}...")
        return None

    # Date/Time
    date_str = ""
    time_str = ""

    meta_date = soup.find("meta", {"property": "article:published_time"})
    if meta_date:
        content = meta_date.get("content", "")
        date_str = thai_date_to_standard(content)
        t_match = re.search(r"T(\d{2}):(\d{2})", content)
        if t_match:
            time_str = f"{t_match.group(1)}:{t_match.group(2)}"

    # Summary
    summary = ""
    meta_desc = soup.find("meta", {"name": "description"})
    if meta_desc:
        summary = meta_desc.get("content", "").strip()

    # Tags
    tags = []
    for tag_a in soup.find_all("a", href=re.compile(r"/tag/")):
        t = tag_a.get_text(strip=True)
        if t and len(t) < 50:
            tags.append(t)

    print(f"    ‚úÖ {title[:60]}...")

    return {
        "title": title,
        "summary": summary[:500] if summary else "",
        "date": date_str,
        "time": time_str,
        "source": source_name,
        "sourceUrl": url,
        "tags": tags[:8],
    }


def scrape_thaipbs(pages=2, keyword=None):
    """
    Scrape Thai PBS (thaipbs.or.th)
    - /news/categories/politics section
    - Extracts time from <time datetime="..."> tags and JSON-LD datePublished
    """
    articles = []
    source_name = "Thai PBS"

    if keyword:
        urls = [
            f"https://www.thaipbs.or.th/search?q={quote(keyword)}&page={p}"
            for p in range(1, pages + 1)
        ]
    else:
        urls = [
            f"https://www.thaipbs.or.th/news/categories/politics"
        ]

    for url in urls:
        print(f"  üì∞ Fetching: {url}")
        resp = safe_request(url)
        if not resp:
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        article_links = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            # Match /news/content/XXXXXX pattern
            if re.search(r"/news/content/\d+", href):
                full_url = urljoin("https://www.thaipbs.or.th", href)
                article_links.add(full_url)
            # Also match /program/*/watch/* and /program/*/episodes/*
            elif re.search(r"/program/.+?/(watch|episodes)/", href):
                full_url = urljoin("https://www.thaipbs.or.th", href)
                article_links.add(full_url)

        print(f"    Found {len(article_links)} article links")

        for link in list(article_links)[:30]:
            time.sleep(REQUEST_DELAY)
            article = scrape_thaipbs_article(link, source_name)
            if article:
                articles.append(article)

    return articles


def scrape_thaipbs_article(url, source_name="Thai PBS"):
    """
    Scrape a single Thai PBS article.

    Thai PBS news pages have:
    - <time datetime="2026-02-12T20:01:48+07:00">12 ‡∏Å.‡∏û. 69</time>
    - JSON-LD with datePublished

    Thai PBS program pages have:
    - JSON-LD with datePublished and uploadDate
    """
    resp = safe_request(url)
    if not resp:
        return None

    soup = BeautifulSoup(resp.text, "html.parser")

    # Title
    title_el = soup.find("h1")
    if not title_el:
        return None
    title = title_el.get_text(strip=True)

    if not is_election_related(title):
        print(f"    ‚è≠Ô∏è  Skip (not election): {title[:50]}...")
        return None

    # Date/Time ‚Äî prefer <time> tag, then JSON-LD, then meta
    date_str = ""
    time_str = ""

    # Method 1: <time datetime="..."> tag (most accurate for news pages)
    time_tag = soup.find("time", {"datetime": True})
    if time_tag:
        dt_val = time_tag.get("datetime", "")
        # Parse ISO datetime: 2026-02-12T20:01:48+07:00
        iso_match = re.search(r"(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2})", dt_val)
        if iso_match:
            y, m, d, hh, mm = iso_match.groups()
            be_year = int(y) + 543
            date_str = f"{d}/{m}/{be_year}"
            time_str = f"{hh}:{mm}"

    # Method 2: JSON-LD datePublished (works for both news & program pages)
    if not date_str:
        for script in soup.find_all("script", type="application/ld+json"):
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    dp = data.get("datePublished", "")
                    if dp:
                        iso_match = re.search(
                            r"(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2})", dp
                        )
                        if iso_match:
                            y, m, d, hh, mm = iso_match.groups()
                            be_year = int(y) + 543
                            date_str = f"{d}/{m}/{be_year}"
                            time_str = f"{hh}:{mm}"
                            break
            except (json.JSONDecodeError, TypeError):
                pass

    # Method 3: meta article:published_time
    if not date_str:
        meta_date = soup.find("meta", {"property": "article:published_time"})
        if meta_date:
            content = meta_date.get("content", "")
            date_str = thai_date_to_standard(content)
            t_match = re.search(r"T(\d{2}):(\d{2})", content)
            if t_match:
                time_str = f"{t_match.group(1)}:{t_match.group(2)}"

    # Summary ‚Äî meta description
    summary = ""
    meta_desc = soup.find("meta", {"name": "description"})
    if meta_desc:
        summary = meta_desc.get("content", "").strip()

    if not summary:
        # Try first <p> in article body
        article_body = soup.find("div", class_=re.compile(r"article|content|entry"))
        if article_body:
            first_p = article_body.find("p")
            if first_p:
                summary = first_p.get_text(strip=True)[:300]

    # Tags ‚Äî Thai PBS uses /tags?q= links
    tags = []
    for tag_a in soup.find_all("a", href=re.compile(r"/tags\?q=")):
        t = tag_a.get_text(strip=True)
        if t and len(t) < 50:
            tags.append(t)

    print(f"    ‚úÖ {title[:60]}...")

    return {
        "title": title,
        "summary": summary[:500] if summary else "",
        "date": date_str,
        "time": time_str,
        "source": source_name,
        "sourceUrl": url,
        "tags": tags[:8],
    }


def scrape_pptv(pages=1, keyword=None):
    """
    Scrape PPTV HD36 (pptvhd36.com)
    - /news/‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á section
    """
    articles = []
    source_name = "PPTV HD36"

    base_url = "https://www.pptvhd36.com"

    if keyword:
        urls = [f"{base_url}/search?q={quote(keyword)}"]
    else:
        urls = [f"{base_url}/news/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%80%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%87"]

    for url in urls:
        print(f"  üì∞ Fetching: {url}")
        resp = safe_request(url)
        if not resp:
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        article_links = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if re.search(r"/news/.*?/\d+", href):
                full_url = urljoin(base_url, href)
                article_links.add(full_url)

        print(f"    Found {len(article_links)} article links")

        for link in list(article_links)[:20]:  # Limit to 20 per page
            time.sleep(REQUEST_DELAY)
            article = scrape_pptv_article(link, source_name)
            if article:
                articles.append(article)

    return articles


def scrape_pptv_article(url, source_name="PPTV HD36"):
    """Scrape a single PPTV article."""
    resp = safe_request(url)
    if not resp:
        return None

    soup = BeautifulSoup(resp.text, "html.parser")

    title_el = soup.find("h1")
    if not title_el:
        return None
    title = title_el.get_text(strip=True)

    if not is_election_related(title):
        print(f"    ‚è≠Ô∏è  Skip (not election): {title[:50]}...")
        return None

    date_str = ""
    time_str = ""

    meta_date = soup.find("meta", {"property": "article:published_time"})
    if meta_date:
        content = meta_date.get("content", "")
        date_str = thai_date_to_standard(content)
        t_match = re.search(r"T(\d{2}):(\d{2})", content)
        if t_match:
            time_str = f"{t_match.group(1)}:{t_match.group(2)}"

    summary = ""
    meta_desc = soup.find("meta", {"name": "description"})
    if meta_desc:
        summary = meta_desc.get("content", "").strip()

    tags = []
    for tag_a in soup.find_all("a", href=re.compile(r"/tag/")):
        t = tag_a.get_text(strip=True)
        if t and len(t) < 50:
            tags.append(t)

    print(f"    ‚úÖ {title[:60]}...")

    return {
        "title": title,
        "summary": summary[:500] if summary else "",
        "date": date_str,
        "time": time_str,
        "source": source_name,
        "sourceUrl": url,
        "tags": tags[:8],
    }


# ‚îÄ‚îÄ‚îÄ Deduplication ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def deduplicate(articles):
    """Remove duplicate articles by URL."""
    seen_urls = set()
    unique = []
    for a in articles:
        url = a.get("sourceUrl", "")
        if url not in seen_urls:
            seen_urls.add(url)
            unique.append(a)
    return unique


# ‚îÄ‚îÄ‚îÄ Load existing data to avoid re-scraping ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def load_existing():
    """Load previously scraped articles."""
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data.get("articles", [])
        except Exception:
            pass
    return []


def load_existing_urls_from_tsx():
    """Load URLs already in ElectionNews.tsx to mark them."""
    tsx_path = os.path.join(
        os.path.dirname(__file__), "..", "visualization",
        "src", "components", "ElectionNews.tsx"
    )
    urls = set()
    if os.path.exists(tsx_path):
        with open(tsx_path, "r", encoding="utf-8") as f:
            for line in f:
                match = re.search(r"sourceUrl:\s*['\"](.+?)['\"]", line)
                if match:
                    urls.add(match.group(1))
    return urls


# ‚îÄ‚îÄ‚îÄ Output Formatting ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def format_for_tsx(articles, existing_tsx_urls):
    """
    Format articles as TSX-ready code snippet.
    Marks articles already in ElectionNews.tsx.
    """
    lines = []
    lines.append("// ‚îÄ‚îÄ‚îÄ Scraped News (‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á) ‚îÄ‚îÄ‚îÄ")
    lines.append("// ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà scrape: " + datetime.now().strftime("%Y-%m-%d %H:%M"))
    lines.append("// ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡πÉ‡∏™‡πà‡πÉ‡∏ô NEWS_DATA array")
    lines.append("// ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏° id, category, province, area ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°")
    lines.append("")

    for i, a in enumerate(articles, 1):
        in_tsx = a["sourceUrl"] in existing_tsx_urls
        marker = "// ‚úÖ ALREADY IN TSX" if in_tsx else "// üÜï NEW"

        lines.append(f"  {marker}")
        lines.append("  {")
        lines.append(f"    id: ???,  // ‡∏Å‡∏≥‡∏´‡∏ô‡∏î id ‡πÄ‡∏≠‡∏á")
        lines.append(f"    title: '{_escape(a['title'])}',")
        lines.append(f"    summary: '{_escape(a['summary'])}',")
        lines.append(f"    date: '{a['date']}',")
        if a.get("time"):
            lines.append(f"    time: '{a['time']}',")
        lines.append(f"    source: '{a['source']}',")
        lines.append(f"    sourceUrl: '{a['sourceUrl']}',")
        lines.append(f"    category: '???',  // recount | fraud | irregularity | ect | analysis")
        lines.append(f"    tags: {json.dumps(a.get('tags', []), ensure_ascii=False)},")
        lines.append("  },")
        lines.append("")

    return "\n".join(lines)


def _escape(s):
    """Escape single quotes for TSX string."""
    return s.replace("'", "\\'").replace("\n", " ").strip()


# ‚îÄ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    parser = argparse.ArgumentParser(
        description="üó≥Ô∏è Election 69 News Scraper ‚Äî Scrape ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á"
    )
    parser.add_argument(
        "--pages", type=int, default=2,
        help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞ scrape ‡∏ï‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á (default: 2)"
    )
    parser.add_argument(
        "--source", type=str, default="all",
        choices=["all", "khaosod", "thairath", "matichon", "pptv", "thaipbs"],
        help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß (default: all)"
    )
    parser.add_argument(
        "--keyword", type=str, default=None,
        help="‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ keyword ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô '‡∏ö‡∏≤‡∏£‡πå‡πÇ‡∏Ñ‡πâ‡∏î' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ'"
    )
    parser.add_argument(
        "--no-tsx", action="store_true",
        help="‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á generate TSX snippet"
    )
    args = parser.parse_args()

    print("=" * 60)
    print("üó≥Ô∏è  Election 69 News Scraper")
    print("=" * 60)
    print(f"  Pages: {args.pages} | Source: {args.source} | Keyword: {args.keyword or '(none)'}")
    print()

    all_articles = []

    # Load existing
    existing = load_existing()
    existing_urls = {a["sourceUrl"] for a in existing}
    print(f"üì¶ Existing scraped articles: {len(existing)}")

    # Scrape each source
    scrapers = {
        "khaosod": scrape_khaosod,
        "thairath": scrape_thairath,
        "matichon": scrape_matichon,
        "pptv": scrape_pptv,
        "thaipbs": scrape_thaipbs,
    }

    sources = [args.source] if args.source != "all" else scrapers.keys()

    for source in sources:
        print(f"\n{'‚îÄ' * 40}")
        print(f"üì° Scraping: {source.upper()}")
        print(f"{'‚îÄ' * 40}")

        scraper_fn = scrapers[source]
        new_articles = scraper_fn(pages=args.pages, keyword=args.keyword)
        print(f"  ‚Üí Got {len(new_articles)} election-related articles")
        all_articles.extend(new_articles)

    # Merge with existing + deduplicate
    combined = existing + all_articles
    combined = deduplicate(combined)

    # Sort by date (newest first)
    def sort_key(a):
        d = a.get("date", "")
        t = a.get("time", "00:00")
        # Convert DD/MM/YYYY to sortable format
        parts = d.split("/")
        if len(parts) == 3:
            return f"{parts[2]}{parts[1]}{parts[0]}{t}"
        return d + t
    combined.sort(key=sort_key, reverse=True)

    # Save to JSON
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    output_data = {
        "scraped_at": datetime.now().isoformat(),
        "total": len(combined),
        "new_in_this_run": len(all_articles),
        "articles": combined,
    }
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\n{'=' * 60}")
    print(f"‚úÖ Saved {len(combined)} articles to: {OUTPUT_FILE}")
    print(f"   New in this run: {len(all_articles)}")

    # Generate TSX snippet
    if not args.no_tsx:
        tsx_urls = load_existing_urls_from_tsx()
        tsx_snippet = format_for_tsx(all_articles, tsx_urls)
        tsx_file = os.path.join(OUTPUT_DIR, "news_tsx_snippet.txt")
        with open(tsx_file, "w", encoding="utf-8") as f:
            f.write(tsx_snippet)
        print(f"üìù TSX snippet saved to: {tsx_file}")

        # Count new vs existing
        new_count = sum(1 for a in all_articles if a["sourceUrl"] not in tsx_urls)
        existing_count = len(all_articles) - new_count
        print(f"   üÜï New (not in TSX): {new_count}")
        print(f"   ‚úÖ Already in TSX: {existing_count}")

    print(f"\nüí° Tip: Review {OUTPUT_FILE} ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡πÉ‡∏™‡πà ElectionNews.tsx")
    print()


if __name__ == "__main__":
    main()
